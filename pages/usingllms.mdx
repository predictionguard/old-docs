---
title: Using LLMs
description: Prediction Guard in action
---

# Using LLMs - Prediction Guard in action

We've worked on a wide variety of enterprise LLM use cases across various industries. Throughout this work, we've found the following principles of LLM usage to be critical and transferable. If you are getting started with Prediction Guard or LLMs in general, the below tutorials about LLM usage should help you level up quickly. Each tutorial can be run in Google Colab without any local environment setup. 

1. [Accessing LLMs](usage/accessing) - Use your Prediction Guard access token to run your first text completions
2. [Basic prompting](usage/prompting) - Learn how to prompt these models for autocomplete, zero shot instructions, and few shot (or in context) learning
3. [Prompt engineering](usage/engineering) - Leverage prompt templates and model parameters to hone in on the right workflows
4. [Augmentation and retrieval](usage/augmentation) - Augment your prompts with your own data
5. [Agents](usage/agents) - Create more complex automations with agentic workflows

**Note** - These examples will be given in Python, but the same workflows could be accomplished in other languages via our REST API.