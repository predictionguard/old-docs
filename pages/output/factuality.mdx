---
title: Factuality
description: Controlled and compliant AI applications
---

# Factuality checks

After accounting for consistent output, you might also want to reduce hallucinations or fact check the output. You might be trying to ground your model outputs with external knowledge, but that doesn't always prevent hallucination. You need checks on your outputs to make sure that you don't offend your users or provide incorrect information.

With Prediction Guard, you can add in factuality check with a "flip of a switch" (or configuration parameter). Our factuality check uses a state-of-the-art model trained to predict factuality of LLM output (given the context of your prompt). You can also call the factuality checking functionality directly using the `/factuality` endpoints, which will enable you to configure thresholds and score arbitrary inputs.

## Factuality Check on Text Completions

Let's use the following  prompt template to determine some features of an instragram post announcing new products. First, we can define a prompt template:

```python copy
import os
import json

import predictionguard as pg
from langchain import PromptTemplate

os.environ["PREDICTIONGUARD_TOKEN"] = "<your access token>"

template = """Respond to the following query based on the context.

Context: EVERY comment, DM + email suggestion has led us to this EXCITING announcement! ðŸŽ‰ We have officially added TWO new candle subscription box options! ðŸ“¦
Exclusive Candle Box - $80
Monthly Candle Box - $45 (NEW!)
Scent of The Month Box - $28 (NEW!)
Head to stories to get ALLL the deets on each box! ðŸ‘† BONUS: Save 50% on your first box with code 50OFF! ðŸŽ‰

Query: {query}

Result: """
prompt = PromptTemplate(template=template, input_variables=["query"])
```

Then we can inject a query and add in both a type and a `factuality` check to the `output` configuration. When we add `"factuality": True`, Prediction Guard will check the output for factuality. If the response is suspect, Prediction Guard will return an error status.

```python copy
result = pg.Completion.create(
    model="Camel-5B",
    prompt=prompt.format(query="How many new products are listed?"),
    output={
        "factuality": True
    }
)

print(json.dumps(
    result,
    sort_keys=True,
    indent=4,
    separators=(',', ': ')
))
```

This outputs something similar to:

```json
{
    "choices": [
        {
            "index": 0,
            "output": 2,
            "status": "success",
            "text": "2"
        }
    ],
    "created": 1686858057,
    "id": "cmpl-3MM8uyNCBLP3sroYJiUhBhpPLj66m",
    "model": "Camel-5B",
    "object": "text_completion"
}
```

Now, we could try to make the model hallucinate. However, the hallucination is caught and Prediction Guard returns an error status:

```python copy
result = pg.Completion.create(
    model="Camel-5B",
    prompt=prompt.format(query="How many giraffes are listed?"),
    output={
        "factuality": True
    }
)

print(json.dumps(
    result,
    sort_keys=True,
    indent=4,
    separators=(',', ': ')
))
```

This outputs something similar to:

```json
{
    "choices": [
        {
            "index": 0,
            "status": "error: failed a factuality or toxicity check",
            "text": ""
        }
    ],
    "created": 1686857961,
    "id": "cmpl-WiOOQBy9No5F2OFhUrK24tRnEhcdb",
    "model": "Camel-5B",
    "object": "text_completion"
}
```



## Standalone Factuality functionality

You can also call the factuality checking functionality directly using the [`/factuality`](../reference/factuality) endpoint, which will enable you to configure thresholds and score arbitrary inputs.