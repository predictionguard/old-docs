---
title: Default Models
description: Reliable, future proof AI predictions
---

# Default Models

Although Prediction Guard allows you to create your own customized, domain specific model proxies, we also have
pre-configured proxies for you. You can think about these as "default" models that you can use out-of-the-box for
inference/prediction without creating any model input/ output examples. Just start getting reliable predictions in
a single call!

To create these default model configurations, we have curated internal test sets for all of our tasks. We are continuously 
updating these and re-evaluating the latest models. If you use one of our default model endpoints, you will be getting 
**state-of-the-art model inference for most "general" use cases that updates to the latest models as they are released.**

Our default model configurations can be used with the standard [predict API](reference/predict) by specifying the name
of the proxy as `default-` plus the [task name](tasks). For example to use our default LLM for text generation:

```python
import predictionguard as pg

# Instantiate a prediction guard client
client = pg.Client(token="<your access token>")

# Use the default text-gen model to get a completion
output = client.predict(name="default-text-gen", data={
    "prompt": "Tell me a joke"
})
print(output)
```

This should result in something like: 

```
{'text': '\n\nQ: What did the fish say when it hit the wall?\nA: Dam!'}
```

To use the default model for other tasks, just swap out the task in the proxy name. `default-sentiment`, `default-fact`, etc...