---
title: Controlling LLM Output
description: Controlled and compliant AI applications
---

# Controlling LLM Output

With Prediction Guard, you can control the output of LLMs in the following ways (which can be mixed and matched according to your use case and constraints):

- [Enforce output structure and/or types](output/types)
- [Check for consistency](output/consistency)
- [Filter for factuality ](output/factuality)
- [Filter for toxicity](output/toxicity)
