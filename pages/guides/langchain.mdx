---
title: LangChain Prompts
---

# Setting up text generation with LangChain and Prediction Guard

To configure a custom Prediction Guard endpoint for text generation, you need to generate example prompts. These example
prompts are used by the system to find the best Large Language Model (LLM) for you use case. Currently Prediction Guard
has access to OpenAI (GPT 3.5 etc.), co:here, Cerebras, Hugging Face, and other state-of-the-art models for text generation.
The prompt examples and "proxies" of Prediction Guard allow you to rapidly select and start using these models without the
need to maintain a bunch of different API keys and manually try them all. 

That's great, but you can do even better! You can use [LangChain](https://github.com/hwchase17/langchain) and its prompt
templates to help you quickly setup your prompt examples. 

Let's say that you want to configure a LLM endpoint for zero shot text classification. That is, you want to use a LLM to 
classify text into one of a number of categories on-the-fly. You can first create a LangChain prompt template:

```python
from langchain import PromptTemplate


template = """Classify the following input text into one of the following categories: [{categories}]

Input Text: {text}

Category: """

prompt = PromptTemplate(
    input_variables=["categories", "text"],
    template=template,
)
```

The load a number of example values for `categories` and `text`:

```python
# Each row here includes:
# 1. an example text input (that we want to classify)
# 2. an example list of categories
# 3. the expected output category/ class (that we expect to predict)
zsdata = [
    ["I have successfully booked your tickets.", "agent, customer", "agent"],
    ["What's the oldest building in US?", "quantity, location", "location"],
    ["This video game is amazing. I love it!", "positive, negative", ""],
    ["Dune is the best movie ever.", "cinema, art, music", "cinema"],
    ["I have a problem with my iphone that needs to be resolved asap!", "urgent, not urgent", "urgent"]
]
```

Setting up the Prediction Guard proxy is then as easy as:

```python
import predictionguard as pg
 
# Initialize a Prediction Guard client.
client = pg.Client(token=<your access token>)

# Create the examples using our prompt template
examples = []
for zs in zsdata:
  examples.append({
    "input": {
        "prompt": prompt.format(
            categories=zs[1], 
            text=zs[0]
        ),
        "temperature": 0.1,
        "max_tokens": len(zs[2]) + 20
    },
    "output": {
        "answer": zs[2]
    }
  })

# Create a prediction "proxy." This proxy will save your examples, evaluate
# SOTA models to find the best one for your use case, and expose the best model
# at an endpoint corresponding to the proxy.
client.create_proxy(task='text-gen', name='my-text-gen-proxy', examples=examples)
```

You will see Prediction Guard configuring your proxy (it should take less than 5 minutes). As soon as model selection
is complete, you can start using your configured LLM-driven zero shot classification endpoint:

```python
# Now your ready to start providing new input!
new_input = "Thanks for booking my tickets!"
prediction = client.predict(name='my-text-gen-proxy', data={
        "prompt": prompt.format(
            categories="customer, agent", 
            text=new_input
        ),
        "temperature": 0.1,
        "max_tokens": 10
})

print(prediction)
```