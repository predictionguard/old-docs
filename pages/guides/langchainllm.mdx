---
title: LangChain LLM Wrapper
---

# Using Prediction Guard proxies in LangChain

[LangChain](https://github.com/hwchase17/langchain) is one of the most popular AI projects, and for good reason! LangChain
helps you "Build applications with LLMs through composability." LangChain doesn't solve the problems of model 
reliability, use case/ domain configuration, or future proofing that are addressed by Prediction Guard. Therefore, combining 
the two (Prediction Guard + LangChain) gives you **a framework for developing reliable, future proofed applications powered 
by language models.**

## LLM Wrapper

There exists a Prediction Guard LLM wrapper, which you can access with 
```python copy
from langchain.llms import PredictionGuard
```

You can provide the name of your Prediction Guard "proxy" as an argument when initializing the LLM:
```python copy
pgllm = PredictionGuard(name="your-text-gen-proxy")
```

Alternatively, you can use Prediction Guard's default proxy for SOTA LLMs:
```python copy
pgllm = PredictionGuard(name="default-text-gen")
```

You can also provide your access token directly as an argument:
```python copy
pgllm = PredictionGuard(name="default-text-gen", token="<your access token>")
```

## Basic LLM usage

To use the Prediction Guard [default `text-gen` proxy](../defaults) as an LLM:

```python copy
from langchain.llms import PredictionGuard

pgllm = PredictionGuard(name="default-text-gen", token="<your access token>")
pgllm("Tell me a joke")
```

import { Callout } from 'nextra-theme-docs'

<Callout type="info" emoji="ℹ️">
  If you prefer, you can define your access token via the environmental variable `PREDICTIONGUARD_TOKEN` and remove the `token` argument. 
</Callout>

This should return something like:

```
\n\nQ: What did the fish say when he hit the wall? \nA: Dam!
```

## Chaining

Now to use a Prediction Guard proxy as the LLM in a chaining operation, just supply the initialized LLM object to 
LangChain as follows:

```python copy
from langchain import PromptTemplate, LLMChain

template = """Question: {question}

Answer: Let's think step by step."""
prompt = PromptTemplate(template=template, input_variables=["question"])
llm_chain = LLMChain(prompt=prompt, llm=pgllm, verbose=True)

question = "What NFL team won the Super Bowl in the year Justin Beiber was born?"

llm_chain.predict(question=question)
```

This should return something like:

```
> Entering new LLMChain chain...
Prompt after formatting:
Question: What NFL team won the Super Bowl in the year Justin Beiber was born?

Answer: Let's think step by step.

> Finished chain.
 Justin Bieber was born in 1994, so the NFL team that won the Super Bowl in 1994 was the Dallas Cowboys.
```
