
title: ManyChat Example

Using Prediction Guard to provide responses in ManyChat

ManyChat is a popular chat automation tool for customer service. This guide covers two methods to integrate LLM responses using the Prediction Guard API. The first method is straightforward, involving a single question without context, which can be set up in 5-10 minutes without coding. The second method requires setting up a lambda function to process the entire chat request, send it to Prediction Guard, and return the response to ManyChat using their dynamic block. It's possible to handle context within ManyChat exclusively, but this requires extensive manual work.

Prerequisites:

Prediction Guard API Key
Sign up for ManyChat (Premium account required)
Create two ManyChat User Custom fields (User_Question1, Bot_Response1)
Create a Telegram bot
No context example:

Create a new automation and select a context to trigger it.
Create a Telegram Menu with a "Question?" button, which sends the user's question to Prediction Guard.
Add a User Input block for the user to input their question and save the response to a custom field.
Add an External Request Action Block to send the question to Prediction Guard and clear the Bot_Response1 field.
Format the HTTP Post request as per Prediction Guard's documentation, including headers and body.
Test the response and map it to the Bot_Response1 custom field.
Create a new Telegram Message Block to display the response and include a menu with options to ask a new question, talk to an agent, or close the conversation.
Include Conversation Context Example:

This method includes the context of previous questions.

Follow steps 1-6 of the basic example.
Use a "Get Dynamic Content" block instead of an External Request Action.
Send user data to a Lambda function.
Build the Lambda function to process the request, format it for Prediction Guard, send the API request, process the response, append it to the conversation history, format the ManyChat response, and handle errors.
Ensure the flow allows the customer to continue the conversation, ask new questions (clearing context), or reach a human agent.
These methods enable ManyChat to leverage Prediction Guard's LLM capabilities, enhancing customer service with automated, context-aware responses.
