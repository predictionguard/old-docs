---
title: Custom Output Structuring in LLMs
---

# Alternative Output Structuring with PredictionGuard

This example demonstrates how to integrate PredictionGuard with custom data structures for output parsing, utilizing the Pydantic library. The focus is on structuring outputs in a specific format, in this case, a joke, ensuring the setup ends with a question mark.

## Requirements

- PredictionGuard API
- Pydantic library
- LangChain library

## Implementation Steps

1. **Set Environment Variable for PredictionGuard**: Ensure that your PredictionGuard API token is correctly set up in your environment variables.

```python
import os
os.environ["PREDICTIONGUARD_TOKEN"] = "<your access token>"
```

2. **Import Necessary Libraries**: Import PredictionGuard, Pydantic for data validation, and LangChain for output parsing and prompt templating.

```python
import predictionguard as pg
from langchain.output_parsers import PydanticOutputParser
from langchain.prompts import PromptTemplate
from pydantic import BaseModel, Field, validator
```

3. **Define Your Data Structure**: Create a Pydantic model to define the structure of a joke, including setup and punchline, with a validator to ensure the setup is properly formed.

```python
class Joke(BaseModel):
    setup: str = Field(description="question to set up a joke")
    punchline: str = Field(description="answer to resolve the joke")

    @validator("setup")
    def question_ends_with_question_mark(cls, field):
        if not field.endswith("?"):
            raise ValueError("Badly formed question!")
        return field
```

4. **Set Up an Output Parser**: Utilize LangChain's `PydanticOutputParser` to enforce the data structure defined by the Pydantic model on the output.

```python
parser = PydanticOutputParser(pydantic_object=Joke)
```

5. **Create and Format a Prompt**: Use LangChain's `PromptTemplate` to structure your query, incorporating instructions for output formatting derived from the parser.

```python
prompt = PromptTemplate(
    template="Answer the user query.\n{format_instructions}\n{query}\n",
    input_variables=["query"],
    partial_variables={"format_instructions": parser.get_format_instructions()},
)
```

6. **Generate and Parse Output**: Call PredictionGuard's text completion model "Neural-Chat-7B" to generate an output based on the formatted prompt, then parse the output into the Pydantic model. Handle exceptions for parsing errors.

```python
result = pg.Completion.create(
    model="Neural-Chat-7B",
    prompt=prompt.format(query="Tell me a joke."),
    max_tokens=200,
    temperature=0.1
)

try:
    joke = Joke.parse_raw(result['choices'][0]['text'])
    print(joke.setup)
    print(joke.punchline)
except Exception as e:
    print(f"Error parsing joke: {e}")
```
## Let's Test This Out

After running the implementation with the query "Tell me a joke.", the structured output generated by PredictionGuard, parsed and validated by our Pydantic model, looks like this:

```json
{
  "setup": "Why did the tomato turn red?",
  "punchline": "Because it saw the salad dressing."
}
```

**Output Explanation**:

- **Setup**: "Why did the tomato turn red?" - This question successfully passes the validator check, ending with a question mark as required.
- **Punchline**: "Because it saw the salad dressing." - Provides a humorous answer to the setup question.

This example demonstrates the effective structuring and validation of output data, ensuring that the generated joke adheres to our defined format.

**Conclusion**:

This approach allows for the flexible and structured generation of outputs, leveraging PredictionGuard's capabilities alongside Pydantic's validation features.

