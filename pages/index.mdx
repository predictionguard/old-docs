---
title: Getting Started
description: Reliable, future proof AI predictions
---

# Getting Started

Technical teams need to figure out how to integrate the latest Large Language Models (LLMs), but:
- You can’t build robust systems with inconsistent, unvalidated outputs; and
- LLM integrations scare corporate lawyers, finance departments, and security professionals due to hallucinations, cost, lack of compliance (e.g., HIPAA), leaked IP/PII, and “injection” vulnerabilities.

Some companies are moving forward anyway by investing tons of engineering time/money in their own wrappers around LLMs and expensive hosting with OpenAI/Azure. Others are ignoring these issues and pressing forward with fragile and risky LLM integrations. 

At Prediction Guard, we think that you should get useful output from compliant AI systems (without crazy implementation/ hosting costs), so our solution lets you:
1. **De-risk LLM inputs** to remove PII and prompt injections; 
2. **Validate and check LLM outputs** to guard against hallucination, toxicity and inconsistencies; and
3. **Implement private and compliant LLM systems** (HIPAA and self-hosted) that give your legal counsel warm fuzzy feeling while still delighting your customers with AI features.

Sounds pretty great right? Follow the steps below to starting leveraging trustworthy LLMs:

## 1. Get access to Prediction Guard Enterprise

import { Callout } from 'nextra-theme-docs'
 
We host and control the latest LLMs for you in our enterprise platform, so you can focus on your prompts and chains. To access the hosted LLMs (and our interfaces to closed LLMs like OpenAI), contact us about enterprise [here](https://www.predictionguard.com/getting-started). 

After setting up your enterprise account, you will receive one or more Prediction Guard access tokens. You will need this access token to continue.

## 2. (Optional) Install the Python client

You can configure and use Prediction Guard using our Python client or via REST API directly. If you are wanting to use the Python client, you can install it as follows:

```sh
$ pip install predictionguard
```

## 3. Start using one of our several LLMs!

Suppose you want to prompt an LLM to answer a user query based on the role that is assigned to it:

```
[
    {
        "role": "system",
        "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company."
    },
    {
        "role": "user",
        "content": "Where can I access the LLMs in a safe and secure environment?"
    }
]
```

You can use our Python client or REST API to prompt one of many open or closed LLMs. Our API is very similar to the OpenAI API for text completion, but we've added an `output` argument/field that let's you perform checks on the output of LLMs. 

import { Tab, Tabs } from 'nextra-theme-docs'

<Tabs items={['Python', 'Go', 'NodeJS', 'cURL']}  defaultIndex="0">
  <Tab>
    ```python filename="main.py" copy
    import os
    import json

    import predictionguard as pg

    # Set your Prediction Guard token as an environmental variable.
    os.environ["PREDICTIONGUARD_TOKEN"] = "<your access token>"

    # Define our prompt.
    prompt = """[
    {
        "role": "system",
        "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company. You will only answer the questions and not provide any additional information"
    },
    {
        "role": "user",
        "content": "Where can I access the LLMs in a safe and secure environment?"
    }
]"""

    
result = pg.Completion.create(
    model="Notus-7B",
    prompt=prompt, 
    output={
        "consistency": True
    }

)

print(json.dumps(
    result,
    sort_keys=True,
    indent=4,
    separators=(',', ': ')
))
    ```
  </Tab>
  <Tab>
    ```go filename="prediction.go" copy
    package main

    import (
      "fmt"
      "strings"
      "net/http"
      "io/ioutil"
    )

    func main() {

      url := "https://api.predictionguard.com/completions"
      method := "POST"

      payload := strings.NewReader(`{
        "model": "Notus-7B",
        "prompt": """[
    {
        "role": "system",
        "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company."
    },
    {
        "role": "user",
        "content": "Where can I access the LLMs in a safe and secure environment?"
    }
]""",
        "output": {
            "consistency": true
        }
    }`)

      client := &http.Client{}
      req, err := http.NewRequest(method, url, payload)

      if err != nil {
        fmt.Println(err)
        return
      }
      req.Header.Add("Authorization", "Bearer <your access token>")
      req.Header.Add("Content-Type", "application/json")

      res, err := client.Do(req)
      if err != nil {
        fmt.Println(err)
        return
      }
      defer res.Body.Close()

      body, err := ioutil.ReadAll(res.Body)
      if err != nil {
        fmt.Println(err)
        return
      }
      fmt.Println(string(body))
    }
    ```
  </Tab>
  <Tab>
    ```js copy
    var axios = require('axios');
    var data = JSON.stringify({
      "model": "Notus-7B",
      "prompt": "[
    {
        "role": "system",
        "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company. You will only answer the questions and not provide any additional information"
    },
    {
        "role": "user",
        "content": "Where can I access the LLMs in a safe and secure environment?"
    }
] ",
      "output": {
        "consistency": True
      }
    });

    var config = {
      method: 'post',
      url: 'https://api.predictionguard.com/completions',
      headers: { 
        'Authorization': 'Bearer <your access key>', 
        'Content-Type': 'application/json'
      },
      data : data
    };

    axios(config)
    .then(function (response) {
      console.log(JSON.stringify(response.data));
    })
    .catch(function (error) {
      console.log(error);
    });
    ```
  </Tab>
  <Tab>
    ```bash copy
    $ curl --location --request POST 'https://api.predictionguard.com/completions' \
    --header 'x-api-key: <your access token>' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "model": "Notus-7B",
        "prompt": "[
    {
        "role": "system",
        "content": "You are a helpful assistant. Your model is hosted by Prediction Guard, a leading AI company. You will only answer the questions and not provide any additional information"
    },
    {
        "role": "user",
        "content": "Where can I access the LLMs in a safe and secure environment?"
    }
]",
        "output": {
            "consistency": "True"
        }
    }'
    ```
  </Tab>
</Tabs>

<Callout type="info" emoji="ℹ️">
  Note, you will need to replace `<your access token>` in the above examples with your actual access token retrieved
  from [your user dashboard](https://predictionguard.com/app).
</Callout>

Thus, this should result in the following output. The `choices[0].text` field contains the raw LLM output. It also contains the ["consistency"](https://docs.predictionguard.com/output/consistency) check to ensure that the model ouput is consistent. 

```json
{
    "choices": [
        {
            "index": 0,
            "model": "Notus-7B",
            "status": "success",
            "text": "\n\n# Define the response\nresponse = \"You can access the LLMs in a safe and secure environment by using Prediction Guard's AI Marketplace. Our platform provides a trusted environment for deploying and managing AI models, including LLMs, with features such as model versioning, access control, and monitoring. Additionally, our platform provides a range of tools and services to help you train, fine-tune, and customize your LLMs to meet your specific needs.\"\n\n"
        }
    ],
    "created": 1701875516,
    "id": "cmpl-5RYj7aYsAWkzTutDYWC4JmdZyMWi6",
    "object": "text_completion"
}
```



<Callout type="info" emoji="ℹ️">
  Note, Prediction Guard's models run on serverless infrastructure. If you aren't actively using models, they are scaled down. As such, your first call to a model might need to "wake up" that model inference server. You will get a message "Waking up model. Try again in a few minutes." in such cases. Typically it takes around 5-15 minutes to wake up the model server depending on the size of the model. We are actively working on reducing these cold start times. 
</Callout>

## 4. Explore other models, and guides

This is only the beginning of what your can do with Prediction Guard (not to mention what is on our roadmap). Now that you have a working example, consider exploring:

- [Other output types and checks](output) to further control and validate LLM outputs
- The [variety of models available](models) via Prediction Guard's easy-to-use API
- One of our [guides](guides) (which walk you through example applications)
