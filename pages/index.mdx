---
title: Getting Started
description: Reliable, future proof AI predictions
---

# Getting Started

You no longer need to search through a staggering number of state-of-the-art (SOTA) models to find the 
best one for your use case. In fact, you can stop worrying about models, keeping up with the latest releases,
and buggy research implementations altogether. 

With Prediction Guard you:

- Create examples of the input and the output you expect
- Send those examples to Prediction Guard

Prediction Guard then:

- Searches through a host of SOTA models to find the best match for your expected input/output behavior
- Configures an inference endpoint (aka a "proxy") for you to use the best model

You can then immediately start using the proxy to get new inferences! Moreover, Prediction Guard will periodically (around once
a week) re-evaluate your input/output examples in light of new models. If a better model comes on to the scene, you're
proxy endpoint will automatically start using the better model. Now your application is future proofed!

Sounds pretty great right? Follow the steps below to create your first Prediction Guard proxy:

## 1. Create your account

import { Callout } from 'nextra-theme-docs'
 
Sign up for a Prediction Guard account [here](https://beta.predictionguard.com/auth/signup). If you signed up via email/password
you will need to verify your email. Check your email for the verification link.

After verifying your email, you will need to [pick a subscription level](https://beta.predictionguard.com/#pricing) 
that makes sense for you. Our pricing is simple. Pay for a number of prediction endpoints (e.g., one for 
text generation and one for question answering) and the rate limit you require (# of requested predictions or inferences per second).

**You do NOT need to have access tokens to all the various AI platforms (OpenAI, co:here, etc.) or do any model 
hosting yourself.** We take care of all that, and our infrastructure is serverless. That mean you don't pay for CPU/GPU
time. You just pay a fixed fee for the volume of requests that makes sense for you.

<Callout type="info" emoji="ℹ️">
  If you need a higher rate limit and are scaling up, that's totally possible. Just contact us at **contact@predictionguard.com**. 
</Callout>

## 2. Grab an access token

Once you are signed up (and signed in), head over to the [access tokens page](https://beta.predictionguard.com/app). Click
on the "Add Token" button to add and name a new access token. Click on the eye icon to reveal the newly created access
token. You will need this in the next step.

## 3. (Optional) Install the Python client

You can configure and use Prediction Guard using our Python client or via REST API directly. If you are wanting to use the Python client, you can install it as follows:

```sh
$ pip install predictionguard
```

## 4. Make your first prediction!

Although prediction guard allows you to optimize and future proof your own inference endpoints (or proxies), you can also
use Prediction Guard's "default" models to make predictions without any further configuration. It's as easy as calling
our [predict API](reference/predict) with a `default-` proxy name. 

Try out the following to quickly SOTA LLM output before we move on to configure a customized model proxy:

<Tabs items={['Python', 'Go', 'JavaScript', 'cURL']}  defaultIndex="0">
  <Tab>
    ```python filename="main.py" copy
    import predictionguard as pg

    # Initialize a Prediction Guard client.
    client = pg.Client(token=<your access token>)
    
    # Use the default text-gen model to get a completion
    output = client.predict(name="default-text-gen", data={
        "prompt": "Tell me a joke"
    })
    print(output)
    ```
  </Tab>
  <Tab>
    ```go filename="prediction.go" copy
    package main

    import (
      "fmt"
      "strings"
      "net/http"
      "io/ioutil"
    )

    func main() {

      url := "https://api.predictionguard.com/predict?name=default-text-gen"
      method := "POST"

      payload := strings.NewReader(`{
          "prompt": "Tell me a joke"
      }`)

      client := &http.Client {
      }
      req, err := http.NewRequest(method, url, payload)

      if err != nil {
        fmt.Println(err)
        return
      }
      req.Header.Add("Authorization", "Bearer <your access token>")
      req.Header.Add("Content-Type", "application/json")

      res, err := client.Do(req)
      if err != nil {
        fmt.Println(err)
        return
      }
      defer res.Body.Close()

      body, err := ioutil.ReadAll(res.Body)
      if err != nil {
        fmt.Println(err)
        return
      }
      fmt.Println(string(body))
    }
    ```
  </Tab>
  <Tab>
    ```js filename="predict.js" copy
    var myHeaders = new Headers();
    myHeaders.append("Authorization", "Bearer <your access token>");
    myHeaders.append("Content-Type", "application/json");

    var raw = JSON.stringify({
        "prompt": "Tell me a joke"
    });

    var requestOptions = {
      method: 'POST',
      headers: myHeaders,
      body: raw,
      redirect: 'follow'
    };

    fetch("https://api.predictionguard.com/predict?name=default-text-gen", requestOptions)
      .then(response => response.text())
      .then(result => console.log(result))
      .catch(error => console.log('error', error));
    ```
  </Tab>
  <Tab>
    ```bash copy
    $ curl --location --request POST 'https://api.predictionguard.com/predict?name=default-text-gen' \
    --header 'Authorization: Bearer <your access token>' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "prompt": "Tell me a joke"
    }'
    ```
  </Tab>
</Tabs>

<Callout type="info" emoji="ℹ️">
  Note, you will need to replace `<your access token>` in the above examples with your actual access token retrieved
  from [your user dashboard](https://predictionguard.com/app).
</Callout>

This should result in something like the following output:

```
{'text': '\n\nQ: What did the fish say when it hit the wall?\nA: Dam!'}
```

## 5. Create your first proxy (inference endpoint)

Prediction Guard allows you to make predictions, or inferences, using a customized "proxy" in addition to our
pre-configured default models. These endpoints proxy a bunch 
of SOTA models on the backend (hence the name), and ensure the best possible output.

To create your first proxy for a sentiment analysis task you need to construct example inputs and outputs that match
your domain or use case. Under the hood, Prediction Guard uses these examples to test many different models. Some of these
models, given your input, will produce better results than others. 

When you send your examples to Prediction Guard, the system: 

1. Runs your example inputs through many models in parallel
2. Ranks the model outputs according to how well they match your provided example outputs
3. Selects the top ranking model as a default model for your task and selects the runner up as a fallback model for your task
4. Configures an inference endpoint with the selected models, such that you can start making new inferences/ predictions (via Python or REST API)

<Callout type="info" emoji="ℹ️">
  See the [tasks section](/tasks) to see how you can provide examples for a variety of tasks. See the [models page](/models) if you are curious about the many models that Prediction Guard implements.  
</Callout>

import { Tab, Tabs } from 'nextra-theme-docs'
 
<Tabs items={['Python', 'Go', 'JavaScript - Fetch', 'cURL']}  defaultIndex="0">
  <Tab>
    ```python filename="main.py" copy
    import predictionguard as pg

    # Initialize a Prediction Guard client.
    client = pg.Client(token=<your access token>)

    # Create some examples illustrating the kind of predictions you
    # want to make (domain/ use case specific).
    examples = [
        {
            "input": {
                "phrase": "I'm so excited about Prediction Guard. It's gonna be awesome!"
            },
            "output": {
                "sentiment": "POS"
            }
        },
        {
            "input": {
                "phrase": "AI development without Prediction Guard is bad. It's really terrible."
            },
            "output": {
                "sentiment": "NEG"
            }
        }
    ]

    # Create a prediction "proxy." This proxy will save your examples, evaluate
    # SOTA models to find the best one for your use case, and expose the best model
    # at an endpoint corresponding to the proxy.
    client.create_proxy(task='sentiment', name='my-sentiment-proxy', examples=examples)
    ```
  </Tab>
  <Tab>
    ```go filename="proxy.go" copy
    package main

    import (
      "fmt"
      "strings"
      "net/http"
      "io/ioutil"
    )

    func main() {

      url := "https://api.predictionguard.com/proxy?task=sentiment&name=my-sentiment-proxy"
      method := "POST"

      payload := strings.NewReader(`[
          {
              "input": {
                  "phrase": "I'm so excited about Prediction Guard. It's gonna be awesome!"
              },
              "output": {
                  "sentiment": "POS"
              }
          },
          {
              "input": {
                  "phrase": "AI development without Prediction Guard is bad. It's really terrible."
              },
              "output": {
                  "sentiment": "NEG"
              }
          }
      ]`)

      client := &http.Client {
      }
      req, err := http.NewRequest(method, url, payload)

      if err != nil {
        fmt.Println(err)
        return
      }
      req.Header.Add("Authorization", "Bearer <your access token>")
      req.Header.Add("Content-Type", "application/json")

      res, err := client.Do(req)
      if err != nil {
        fmt.Println(err)
        return
      }
      defer res.Body.Close()

      body, err := ioutil.ReadAll(res.Body)
      if err != nil {
        fmt.Println(err)
        return
      }
      fmt.Println(string(body))
    }
    ```
  </Tab>
  <Tab>
    ```js filename="proxy.js" copy
    var myHeaders = new Headers();
    myHeaders.append("Authorization", "Bearer <your access token>");
    myHeaders.append("Content-Type", "application/json");

    var raw = JSON.stringify([
      {
        "input": {
          "phrase": "I'm so excited about Prediction Guard. It's gonna be awesome!"
        },
        "output": {
          "sentiment": "POS"
        }
      },
      {
        "input": {
          "phrase": "AI development without Prediction Guard is bad. It's really terrible."
        },
        "output": {
          "sentiment": "NEG"
        }
      }
    ]);

    var requestOptions = {
      method: 'POST',
      headers: myHeaders,
      body: raw,
      redirect: 'follow'
    };

    fetch("https://api.predictionguard.com/proxy?task=sentiment&name=my-sentiment-proxy", requestOptions)
      .then(response => response.text())
      .then(result => console.log(result))
      .catch(error => console.log('error', error));
    ```
  </Tab>
  <Tab>
    ```bash copy
    $ curl --location --request POST 'https://api.predictionguard.com/proxy?task=sentiment&name=my-sentiment-proxy' \
    --header 'Authorization: Bearer <your access token>' \
    --header 'Content-Type: application/json' \
    --data-raw '[
        {
            "input": {
                "phrase": "I'\''m so excited about Prediction Guard. It'\''s gonna be awesome!"
            },
            "output": {
                "sentiment": "POS"
            }
        },
        {
            "input": {
                "phrase": "AI development without Prediction Guard is bad. It'\''s really terrible."
            },
            "output": {
                "sentiment": "NEG"
            }
        }
    ]'
    ```
  </Tab>
</Tabs>

## 6. Start making customized predictions!

It will take around 5 seconds to 5 minutes for your proxy endpoint to become "available." You can check the status by 
listing your proxies, as further discussed [here](./reference/proxy). If you are using the python client, you will seeing a
spinner while Prediction Guard finds you a model and configures your endpoint. 

Once available, you can start using the proxy to run new inferences/ predictions:
 
<Tabs items={['Python', 'Go', 'JavaScript', 'cURL']}  defaultIndex="0">
  <Tab>
    ```python filename="main.py" copy
    # Now your ready to start getting reliable, future proof predictions. No fuss!
    prediction = client.predict(name='test-client-sentiment', data={
        "phrase": "This is great! I'm so happy I'm using Prediction Guard"
    })

    print(prediction)
    ```
  </Tab>
  <Tab>
    ```go filename="prediction.go" copy
    package main

    import (
      "fmt"
      "strings"
      "net/http"
      "io/ioutil"
    )

    func main() {

      url := "https://api.predictionguard.com/predict?name=my-sentiment-proxy"
      method := "POST"

      payload := strings.NewReader(`{
          "phrase": "This is great! I'm so happy I'm using Prediction Guard"
      }`)

      client := &http.Client {
      }
      req, err := http.NewRequest(method, url, payload)

      if err != nil {
        fmt.Println(err)
        return
      }
      req.Header.Add("Authorization", "Bearer <your access token>")
      req.Header.Add("Content-Type", "application/json")

      res, err := client.Do(req)
      if err != nil {
        fmt.Println(err)
        return
      }
      defer res.Body.Close()

      body, err := ioutil.ReadAll(res.Body)
      if err != nil {
        fmt.Println(err)
        return
      }
      fmt.Println(string(body))
    }
    ```
  </Tab>
  <Tab>
    ```js filename="predict.js" copy
    var myHeaders = new Headers();
    myHeaders.append("Authorization", "Bearer <your access token>");
    myHeaders.append("Content-Type", "application/json");

    var raw = JSON.stringify({
      "phrase": "This is great! I'm so happy I'm using Prediction Guard"
    });

    var requestOptions = {
      method: 'POST',
      headers: myHeaders,
      body: raw,
      redirect: 'follow'
    };

    fetch("https://api.predictionguard.com/predict?name=my-sentiment-proxy", requestOptions)
      .then(response => response.text())
      .then(result => console.log(result))
      .catch(error => console.log('error', error));
    ```
  </Tab>
  <Tab>
    ```bash copy
    $ curl --location --request POST 'https://api.predictionguard.com/predict?name=my-sentiment-proxy' \
    --header 'Authorization: Bearer <your access token>' \
    --header 'Content-Type: application/json' \
    --data-raw '{
        "phrase": "This is great! I'\''m so happy I'\''m using Prediction Guard"
    }'
    ```
  </Tab>
</Tabs>
